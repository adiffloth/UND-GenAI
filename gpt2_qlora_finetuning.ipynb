{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AABCY8_rFq3_"
      },
      "source": [
        "# GPT-2 QLoRA Fine-Tuning on Rotten Tomatoes data set\n",
        "\n",
        "This notebook will:\n",
        "\n",
        "1. Load a movie review data set from Hugging Face\n",
        "2. Load the pre-trained GPT-2 model\n",
        "3. Perform a baseline evaluation of GPT-2's classification performance\n",
        "4. Perform QLoRA fine-tuning using Bits and Bytes\n",
        "5. Perform an evaluation of the fine-tuned model's classification performance\n",
        "\n",
        "The `cornell-movie-review-data/rotten_tomatoes` data set consists of two columns: the text of a movie review and a binary label that indicates whether the review is positive or negative. There are 8500 records in the train split, 1000 records in the validation split and 1000 records in the test split. The labels are evenly split between positive and negative cases."
      ],
      "id": "AABCY8_rFq3_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Environment setup"
      ],
      "metadata": {
        "id": "_MIiVS1kEwc1"
      },
      "id": "_MIiVS1kEwc1"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2tUerTzkFq4A"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq accelerate bitsandbytes datasets evaluate peft transformers"
      ],
      "id": "2tUerTzkFq4A"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_tdsbXgpFq4B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorWithPadding,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed\n",
        "    )\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    TaskType,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModel\n",
        "    )"
      ],
      "id": "_tdsbXgpFq4B"
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "gKapjcX8EqS2"
      },
      "id": "gKapjcX8EqS2",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Load and prep data"
      ],
      "metadata": {
        "id": "13W6ziHhE_g9"
      },
      "id": "13W6ziHhE_g9"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMSvCvqOFq4B",
        "outputId": "a1ec881c-6431-4c34-f3cf-189b591a8a7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 8530\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
        "dataset"
      ],
      "id": "lMSvCvqOFq4B"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "26c91e4d84024e6e85febd73d442a0b8",
            "83c771fa21d540c6887c83798ae148d0",
            "5ae996ae94c24c99b9ecb4d6096ef4a0",
            "2d48927ab6fb45cb88830b03d7777cb3",
            "1b7bd45f054444dba4dfdfc843644dc1",
            "d1e48365121e418895ded4e69babc374",
            "3427097719644ff7a482629b5b349755",
            "c56b891efb3f4504b22cefebadf49c42",
            "e8f966120169435db38b3903b40b6821",
            "ec533d7be0db495d9895c8172057b941",
            "78086673138745d7a245084a41c65fff"
          ]
        },
        "id": "JPrWsk3vFq4B",
        "outputId": "9d109a5a-4db8-4f44-83a8-34ad9436c261"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26c91e4d84024e6e85febd73d442a0b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['label', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 8530\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['label', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['label', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=False\n",
        "        )\n",
        "\n",
        "tokenized_datasets = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text']\n",
        "    )\n",
        "tokenized_datasets"
      ],
      "id": "JPrWsk3vFq4B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Evaluate baseline GPT-2 model"
      ],
      "metadata": {
        "id": "7alg21bTH2ZM"
      },
      "id": "7alg21bTH2ZM"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WZI_iglrFq4B"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(\n",
        "    tokenizer=tokenizer,\n",
        "    pad_to_multiple_of=8\n",
        "    )\n",
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)\n",
        "    f1 = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "    return {\"accuracy\": accuracy[\"accuracy\"], \"f1\": f1[\"f1\"]}"
      ],
      "id": "WZI_iglrFq4B"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "eaQ6XiEyFq4B",
        "outputId": "326d72ce-e6fc-472e-886e-df02c2b0d3c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-3818195547.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  baseline_trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='134' max='134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [134/134 00:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 7.024363040924072,\n",
              " 'eval_model_preparation_time': 0.0035,\n",
              " 'eval_accuracy': 0.50093808630394,\n",
              " 'eval_f1': 0.33541458658529155,\n",
              " 'eval_runtime': 6.2507,\n",
              " 'eval_samples_per_second': 170.54,\n",
              " 'eval_steps_per_second': 21.438}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "baseline_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    num_labels=2\n",
        "    )\n",
        "\n",
        "baseline_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "baseline_model.to(device)\n",
        "\n",
        "baseline_args = TrainingArguments(\n",
        "    output_dir=\"gpt2-rotten-baseline\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    dataloader_drop_last=False,\n",
        "    report_to=\"none\"\n",
        "    )\n",
        "\n",
        "baseline_trainer = Trainer(\n",
        "    model=baseline_model,\n",
        "    args=baseline_args,\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "baseline_metrics = baseline_trainer.evaluate()\n",
        "baseline_metrics"
      ],
      "id": "eaQ6XiEyFq4B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline F1 score on the validation split: 0.3354"
      ],
      "metadata": {
        "id": "tWrvsfoWIQr7"
      },
      "id": "tWrvsfoWIQr7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Finetune the baseline model using QLoRA"
      ],
      "metadata": {
        "id": "_8F9oZcaIavN"
      },
      "id": "_8F9oZcaIavN"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5pttkdoFq4B",
        "outputId": "dea1f95b-096d-4242-8b1d-1bb07655214c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 2,360,832 || all params: 126,802,176 || trainable%: 1.8618\n"
          ]
        }
      ],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "    )\n",
        "\n",
        "qlora_base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    num_labels=2,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "qlora_base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "qlora_base_model.gradient_checkpointing_enable()\n",
        "qlora_base_model = prepare_model_for_kbit_training(qlora_base_model)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_CLS\n",
        "    )\n",
        "\n",
        "qlora_model = get_peft_model(qlora_base_model, peft_config)\n",
        "qlora_model.print_trainable_parameters()"
      ],
      "id": "m5pttkdoFq4B"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "V25Qv3WnFq4C",
        "outputId": "5ded7e15-25fe-424a-86b3-919bbca55ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-30426067.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  qlora_trainer = Trainer(\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='801' max='801' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [801/801 09:23, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.405200</td>\n",
              "      <td>0.322051</td>\n",
              "      <td>0.858349</td>\n",
              "      <td>0.858304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.340800</td>\n",
              "      <td>0.312567</td>\n",
              "      <td>0.863977</td>\n",
              "      <td>0.863847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.298500</td>\n",
              "      <td>0.306350</td>\n",
              "      <td>0.862101</td>\n",
              "      <td>0.861985</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=801, training_loss=0.4252317217033305, metrics={'train_runtime': 565.7276, 'train_samples_per_second': 45.234, 'train_steps_per_second': 1.416, 'total_flos': 609464077369344.0, 'train_loss': 0.4252317217033305, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "qlora_args = TrainingArguments(\n",
        "    output_dir=\"gpt2-rotten-qlora\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_ratio=0.1,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        "    logging_first_step=True,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    remove_unused_columns=False,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True\n",
        "    )\n",
        "\n",
        "qlora_trainer = Trainer(\n",
        "    model=qlora_model,\n",
        "    args=qlora_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "train_result = qlora_trainer.train()\n",
        "train_result"
      ],
      "id": "V25Qv3WnFq4C"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best F1 score: 0.8638"
      ],
      "metadata": {
        "id": "w-g-zAF_ImGj"
      },
      "id": "w-g-zAF_ImGj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Save the trained model"
      ],
      "metadata": {
        "id": "-gmpra3UIset"
      },
      "id": "-gmpra3UIset"
    },
    {
      "cell_type": "code",
      "source": [
        "adapter_dir = \"gpt2-rotten-qlora/adapter\"\n",
        "base_model_dir = \"gpt2-rotten-qlora/base\"\n",
        "\n",
        "os.makedirs(adapter_dir, exist_ok=True)\n",
        "os.makedirs(base_model_dir, exist_ok=True)\n",
        "\n",
        "qlora_trainer.model.save_pretrained(adapter_dir)\n",
        "tokenizer.save_pretrained(adapter_dir)\n",
        "\n",
        "qlora_trainer.model.get_base_model().save_pretrained(base_model_dir)\n",
        "\n",
        "print(f\"Saved adapter weights to {adapter_dir}\")\n",
        "print(f\"Saved base model weights to {base_model_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0n3mldJfBDh",
        "outputId": "f2b39837-ebe8-418a-ce0b-8a3a43d0ad5d"
      },
      "id": "a0n3mldJfBDh",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved adapter weights to gpt2-rotten-qlora/adapter\n",
            "Saved base model weights to gpt2-rotten-qlora/base\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Reload the trained model and evaluate it"
      ],
      "metadata": {
        "id": "2GNSF0ktIzdH"
      },
      "id": "2GNSF0ktIzdH"
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "    )\n",
        "\n",
        "reloaded_base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    num_labels=2,\n",
        "    device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "reloaded_base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "reloaded_peft_model = PeftModel.from_pretrained(\n",
        "    reloaded_base_model,\n",
        "    adapter_dir\n",
        "    )\n",
        "\n",
        "merged_model = reloaded_peft_model.merge_and_unload()\n",
        "merged_model.to(device)\n",
        "\n",
        "merged_eval_args = TrainingArguments(\n",
        "    output_dir=\"gpt2-rotten-qlora/merged-eval\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    dataloader_drop_last=False,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False\n",
        "    )\n",
        "\n",
        "merged_trainer = Trainer(\n",
        "    model=merged_model,\n",
        "    args=merged_eval_args,\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "merged_validation_metrics = merged_trainer.evaluate()\n",
        "merged_validation_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "NUAPqDY7fA3j",
        "outputId": "f8d6e78c-33bc-408e-d783-c6ded2fb13f9"
      },
      "id": "NUAPqDY7fA3j",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1906982325.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  merged_trainer = Trainer(\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='134' max='134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [134/134 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.30253323912620544,\n",
              " 'eval_model_preparation_time': 0.0022,\n",
              " 'eval_accuracy': 0.8639774859287055,\n",
              " 'eval_f1': 0.86396778948758,\n",
              " 'eval_runtime': 2.9173,\n",
              " 'eval_samples_per_second': 365.408,\n",
              " 'eval_steps_per_second': 45.933}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1 score for the reloaded model: 0.8639"
      ],
      "metadata": {
        "id": "aJxbTOHxI6Ss"
      },
      "id": "aJxbTOHxI6Ss"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "26c91e4d84024e6e85febd73d442a0b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83c771fa21d540c6887c83798ae148d0",
              "IPY_MODEL_5ae996ae94c24c99b9ecb4d6096ef4a0",
              "IPY_MODEL_2d48927ab6fb45cb88830b03d7777cb3"
            ],
            "layout": "IPY_MODEL_1b7bd45f054444dba4dfdfc843644dc1"
          }
        },
        "83c771fa21d540c6887c83798ae148d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1e48365121e418895ded4e69babc374",
            "placeholder": "​",
            "style": "IPY_MODEL_3427097719644ff7a482629b5b349755",
            "value": "Map: 100%"
          }
        },
        "5ae996ae94c24c99b9ecb4d6096ef4a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c56b891efb3f4504b22cefebadf49c42",
            "max": 1066,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8f966120169435db38b3903b40b6821",
            "value": 1066
          }
        },
        "2d48927ab6fb45cb88830b03d7777cb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec533d7be0db495d9895c8172057b941",
            "placeholder": "​",
            "style": "IPY_MODEL_78086673138745d7a245084a41c65fff",
            "value": " 1066/1066 [00:00&lt;00:00, 3155.09 examples/s]"
          }
        },
        "1b7bd45f054444dba4dfdfc843644dc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1e48365121e418895ded4e69babc374": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3427097719644ff7a482629b5b349755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c56b891efb3f4504b22cefebadf49c42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8f966120169435db38b3903b40b6821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec533d7be0db495d9895c8172057b941": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78086673138745d7a245084a41c65fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}